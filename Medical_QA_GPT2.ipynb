{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tuanng007/medical_qa_gpt2/blob/main/Medical_QA_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpQcXW25vTho"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# @title 1. Install dependencies and setting Kaggle API\n",
        "!pip install -q transformers datasets torch accelerate kaggle gradio sentencepiece pandas fuzzywuzzy[speedup] evaluate rouge-score\n",
        "\n",
        "import os\n",
        "import json\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import random\n",
        "import torch\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import re\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "# Upload kaggle.json\n",
        "print(\"Please upload your's file 'kaggle.json' .\")\n",
        "if not os.path.exists('/root/.kaggle/kaggle.json'):\n",
        "    uploaded = files.upload()\n",
        "    if 'kaggle.json' in uploaded:\n",
        "        print(\"Uploaded 'kaggle.json'. Configuring ...\")\n",
        "        !mkdir -p ~/.kaggle\n",
        "        !mv kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "        os.environ['KAGGLE_CONFIG_DIR'] = \"/root/.kaggle\"\n",
        "        print(\"Kaggle API key configured.\")\n",
        "    else:\n",
        "        print(\"Error: Not found file 'kaggle.json' has uploaded.\")\n",
        "else:\n",
        "    print(\"Kaggle.json 'existed. skip the upload step.\")\n",
        "    os.environ['KAGGLE_CONFIG_DIR'] = \"/root/.kaggle\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKi9dT5ez0L6"
      },
      "outputs": [],
      "source": [
        "# @title 2. Import dataset MedQuAD from Kaggle\n",
        "\n",
        "# Define the Kaggle dataset path\n",
        "dataset_id = \"gpreda/medquad\"\n",
        "\n",
        "# Dataset Directory\n",
        "output_dir = \"medquad_data\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Downloading dataset: {dataset_id}...\")\n",
        "# import dataset from Kaggle to the created folder\n",
        "!kaggle datasets download -d {dataset_id} -p {output_dir} --unzip\n",
        "\n",
        "print(f\"Dataset {dataset_id} downloaded and unzip to folder: {output_dir}\")\n",
        "\n",
        "# List the files in the folder to confirm\n",
        "print(\"\\nFiles in folder dataset:\")\n",
        "!ls {output_dir}\n",
        "\n",
        "csv_file_path = os.path.join(output_dir, 'medquad.csv')\n",
        "\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"Error: Not found {csv_file_path}. Please check the file name in the folder {output_dir}.\")\n",
        "    !ls {output_dir}\n",
        "else:\n",
        "    print(f\"Found {csv_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znkU16tX0dxI"
      },
      "outputs": [],
      "source": [
        "# @title 3. Import necessary dependencies and settings GPU\n",
        "from transformers import (\n",
        "    GPT2TokenizerFast,\n",
        "    GPT2ForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "source": [
        "# @title 4 Preprocessing MedQuAD\n",
        "\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "processed_data_generative = []\n",
        "question_col = 'question'\n",
        "answer_col = 'answer'\n",
        "\n",
        "# Function handling spaces and new lines\n",
        "def normalize_whitespace(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    text = text.replace('\\\\n', ' ')\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Function preprocessing text\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text_no_html = soup.get_text()\n",
        "    text_unescaped = html.unescape(text_no_html)\n",
        "    text_normalized_space = normalize_whitespace(text_unescaped)\n",
        "    text_lowercased = text_normalized_space.lower()\n",
        "\n",
        "    return text_lowercased\n",
        "\n",
        "\n",
        "if 'csv_file_path' in globals() and csv_file_path and os.path.exists(csv_file_path):\n",
        "    try:\n",
        "        print(f\"Reading dataset: {csv_file_path}\")\n",
        "        df_original = pd.read_csv(csv_file_path)\n",
        "\n",
        "        print(\"\\n--- Dataset (before) ---\")\n",
        "        print(f\"Rows and columns: {df_original.shape}\")\n",
        "        # df_original.info()\n",
        "        print(\"\\nFirst five rows of original data:\")\n",
        "        print(df_original.head())\n",
        "        print(f\"\\nInitial number of null questions (NaN): {df_original[question_col].isnull().sum()}\")\n",
        "        print(f\"Initial number of null answers (NaN): {df_original[answer_col].isnull().sum()}\")\n",
        "\n",
        "\n",
        "        # --- START OF PREPROCESSING STEPS ---\n",
        "        df_processed = df_original.copy()\n",
        "        initial_rows_before_any_processing = len(df_processed)\n",
        "\n",
        "        # 1. Remove rows where question or answer are completely NaN (if any)\n",
        "        df_processed.dropna(subset=[question_col, answer_col], inplace=True)\n",
        "        rows_after_dropna = len(df_processed)\n",
        "        print(f\"\\nRemoved {initial_rows_before_any_processing - rows_after_dropna} rows with NaN values in question or answer.\")\n",
        "\n",
        "        # 2. Apply text preprocessing (including HTML stripping, unescape, normalize whitespace, lowercase)\n",
        "        print(\"\\nNormalizing text for 'question' and 'answer' columns...\")\n",
        "        df_processed.loc[:, question_col] = df_processed[question_col].apply(preprocess_text)\n",
        "        df_processed.loc[:, answer_col] = df_processed[answer_col].apply(preprocess_text)\n",
        "        print(\"Text normalization complete.\")\n",
        "\n",
        "        # 3. Remove rows where question or answer become empty AFTER normalization\n",
        "        # (e.g., if initially only contained HTML or whitespace)\n",
        "        df_processed = df_processed[df_processed[question_col].str.strip().astype(bool)]\n",
        "        df_processed = df_processed[df_processed[answer_col].str.strip().astype(bool)]\n",
        "        rows_after_empty_filter = len(df_processed)\n",
        "        print(f\"Removed {rows_after_dropna - rows_after_empty_filter} rows with empty question/answer after normalization.\")\n",
        "\n",
        "\n",
        "        # 4. Remove duplicates BASED ON NORMALIZED COLUMNS\n",
        "        rows_before_final_dedup = len(df_processed)\n",
        "        df_processed.drop_duplicates(subset=[question_col, answer_col], keep='first', inplace=True)\n",
        "        rows_after_final_dedup = len(df_processed)\n",
        "        print(f\"Removed {rows_before_final_dedup - rows_after_final_dedup} duplicate rows after text normalization.\")\n",
        "\n",
        "        print(\"\\n--- DESCRIPTION OF DATA AFTER COMPLETE PREPROCESSING ---\")\n",
        "        print(f\"Final number of rows and columns after all processing steps: {df_processed.shape}\")\n",
        "        print(\"\\nFirst five rows of completely processed data:\")\n",
        "        print(df_processed.head())\n",
        "\n",
        "\n",
        "\n",
        "        for index, row in df_processed.iterrows():\n",
        "\n",
        "            processed_data_generative.append({\n",
        "                \"id\": str(df_original.index[index]) if index in df_original.index else str(index), # Try to keep original ID if possible\n",
        "                \"question\": row[question_col],\n",
        "                \"answer\": row[answer_col]\n",
        "            })\n",
        "\n",
        "        print(f\"\\nNumber of valid Q&A pairs to include in training: {len(processed_data_generative)}\")\n",
        "\n",
        "        if not processed_data_generative:\n",
        "            raise ValueError(\"No data was processed from CSV for training.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing CSV file: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        processed_data_generative = [\n",
        "            {\"id\": \"syn_0\", \"question\": \"what are the symptoms of flu?\", \"answer\": \"symptoms of flu include fever, cough, sore throat, runny or stuffy nose, body aches, headache, chills, and fatigue.\"},\n",
        "            {\"id\": \"syn_1\", \"question\": \"how to treat a common cold?\", \"answer\": \"to treat a common cold, get plenty of rest, drink fluids, and use over-the-counter medications for symptoms.\"},\n",
        "        ]\n",
        "        print(\"Using synthetic data due to processing error.\")\n",
        "else:\n",
        "    if 'csv_file_path' not in globals() or not csv_file_path:\n",
        "        print(f\"CSV file path ('csv_file_path') was not provided (possibly due to an error in Cell 2). Using synthetic data.\")\n",
        "    else: # csv_file_path has a value but the file doesn't exist\n",
        "        print(f\"File {csv_file_path} does not exist. Using synthetic data.\")\n",
        "    # Fallback if no CSV file\n",
        "    processed_data_generative = [\n",
        "        {\"id\": \"syn_0\", \"question\": \"what are the symptoms of flu?\", \"answer\": \"symptoms of flu include fever, cough, sore throat, runny or stuffy nose, body aches, headache, chills, and fatigue.\"},\n",
        "        {\"id\": \"syn_1\", \"question\": \"how to treat a common cold?\", \"answer\": \"to treat a common cold, get plenty of rest, drink fluids, and use over-the-counter medications for symptoms.\"},\n",
        "    ]\n",
        "\n",
        "# (Keep this part from your original code)\n",
        "MAX_SAMPLES_TO_USE = 17000\n",
        "if len(processed_data_generative) > MAX_SAMPLES_TO_USE:\n",
        "    print(f\"\\nLimiting training data to {MAX_SAMPLES_TO_USE} samples for demo.\")\n",
        "    processed_data_generative = random.sample(processed_data_generative, MAX_SAMPLES_TO_USE)\n",
        "elif not processed_data_generative: # Check again if processed_data_generative is empty (e.g., due to fallback error)\n",
        "     print(\"WARNING: processed_data_generative is empty after sampling or due to fallback error. Please check.\")\n",
        "     # Recreate synthetic data if completely empty\n",
        "     processed_data_generative = [\n",
        "        {\"id\": \"syn_fallback_0\", \"question\": \"example question for empty case?\", \"answer\": \"example answer for empty case.\"},\n",
        "    ]\n",
        "\n",
        "if not processed_data_generative: # Final check\n",
        "    raise ValueError(\"No data available for processing (including synthetic data).\")\n",
        "\n",
        "print(f\"\\nFinal number of samples used: {len(processed_data_generative)}\")\n",
        "\n",
        "dataset_gen = Dataset.from_pandas(pd.DataFrame(processed_data_generative))\n",
        "\n",
        "# Split train/validation\n",
        "train_test_split_gen = dataset_gen.train_test_split(test_size=0.1, seed=42) # seed=42 for consistent splitting\n",
        "dataset_dict_gen = DatasetDict({\n",
        "    'train': train_test_split_gen['train'],\n",
        "    'validation': train_test_split_gen['test']\n",
        "})\n",
        "\n",
        "print(\"\\nData structure for Generative QA (after normalization and sampling):\")\n",
        "print(dataset_dict_gen)\n",
        "print(\"\\nExample data sample (from normalized train set):\")\n",
        "if len(dataset_dict_gen[\"train\"]) > 0:\n",
        "    sample_example_gen = dataset_dict_gen[\"train\"][0]\n",
        "    print(f\"ID: {sample_example_gen['id']}\")\n",
        "    print(f\"Question: {sample_example_gen['question']}\")\n",
        "    print(f\"Answer: {sample_example_gen['answer']}\")\n",
        "else:\n",
        "    print(\"Train set is empty!\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ILLR3a87aHt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QasJ3GGG06g3"
      },
      "outputs": [],
      "source": [
        "# @title 5. Import Tokenizer and GPT-2\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model.to(device)\n",
        "print(\"Tokenizer và GPT2LMHeadModel đã được tải.\")\n",
        "print(f\"EOS token: '{tokenizer.eos_token}', ID: {tokenizer.eos_token_id}\")\n",
        "print(f\"PAD token: '{tokenizer.pad_token}', ID: {tokenizer.pad_token_id}\")\n",
        "\n",
        "separator_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h6Dh5yB08ao"
      },
      "outputs": [],
      "source": [
        "# @title 6. Tokenization for trainer (Generative QA - Padding to max_length)\n",
        "\n",
        "effective_max_length = tokenizer.model_max_length\n",
        "\n",
        "print(f\"Effective_max_length: {effective_max_length}\")\n",
        "\n",
        "def preprocess_function_generative(examples):\n",
        "    inputs_text = []\n",
        "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
        "        text = f\"Question: {q} Answer: {a}{tokenizer.eos_token}\"\n",
        "        inputs_text.append(text)\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs_text,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=effective_max_length,\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "column_names_gen_train = dataset_dict_gen[\"train\"].column_names\n",
        "tokenized_datasets_gen = dataset_dict_gen.map(\n",
        "    preprocess_function_generative,\n",
        "    batched=True,\n",
        "    remove_columns=column_names_gen_train\n",
        ")\n",
        "\n",
        "print(\"\\nPadded to max_length:\")\n",
        "sample_tokenized_gen = tokenized_datasets_gen[\"train\"][0]\n",
        "print(f\"Keys: {sample_tokenized_gen.keys()}\")\n",
        "print(f\"Length of Input IDs: {len(sample_tokenized_gen['input_ids'])}\")\n",
        "print(f\"Length of Labels: {len(sample_tokenized_gen['labels'])}\")\n",
        "print(f\"Length of Attention Mask: {len(sample_tokenized_gen['attention_mask'])}\")\n",
        "\n",
        "if len(tokenized_datasets_gen[\"train\"]) > 1:\n",
        "    sample2_tokenized_gen = tokenized_datasets_gen[\"train\"][1]\n",
        "    print(f\"Length of Input IDs (sample 2): {len(sample2_tokenized_gen['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIQEJmVX1Chu"
      },
      "outputs": [],
      "source": [
        "# @title 7. Training (Fine-tuning) GPT 2 (Generative QA)\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "data_collator_gen = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "training_args_gen = TrainingArguments(\n",
        "    output_dir=\"./results_medquad_gpt2_generative\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer_gen = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_gen,\n",
        "    train_dataset=tokenized_datasets_gen[\"train\"],\n",
        "    eval_dataset=tokenized_datasets_gen[\"validation\"],\n",
        "    data_collator=data_collator_gen,\n",
        ")\n",
        "\n",
        "print(\"\\nStarted training Generative QA...\")\n",
        "try:\n",
        "    trainer_gen.train()\n",
        "    print(\"Training completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Try reduce batch size if OOM.\")\n",
        "\n",
        "# Save model and token\n",
        "output_model_dir_gen = \"./fine_tuned_medquad_gpt2_generative_final\"\n",
        "trainer_gen.save_model(output_model_dir_gen)\n",
        "tokenizer.save_pretrained(output_model_dir_gen)\n",
        "print(f\"Model and tokenizer (Generative) have been saved: {output_model_dir_gen}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lMwMcUE4V-H"
      },
      "outputs": [],
      "source": [
        "# @title 8. Model Evaluation (ROUGE)\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "eval_model = trainer_gen.model\n",
        "eval_tokenizer = tokenizer\n",
        "\n",
        "validation_dataset = dataset_dict_gen[\"validation\"]\n",
        "# small_validation_dataset = validation_dataset.select(range(100))\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "print(f\"Started generating answers base on validation ({len(validation_dataset)} ) for evaluate...\")\n",
        "eval_model.eval()\n",
        "eval_model.to(device)\n",
        "\n",
        "for example in tqdm(validation_dataset):\n",
        "    question = example[\"question\"]\n",
        "    reference_answer = example[\"answer\"]\n",
        "\n",
        "    prompt = f\"Question: {question} Answer:\"\n",
        "    inputs = eval_tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "\n",
        "            output_sequences = eval_model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                max_length=len(inputs.input_ids[0]) + 150,\n",
        "                num_beams=3,\n",
        "                no_repeat_ngram_size=2,\n",
        "                early_stopping=True,\n",
        "                eos_token_id=eval_tokenizer.eos_token_id,\n",
        "                pad_token_id=eval_tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "\n",
        "        generated_text_full = eval_tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
        "        answer_prefix = \"Answer:\"\n",
        "        start_of_answer_marker = generated_text_full.rfind(answer_prefix)\n",
        "\n",
        "        if start_of_answer_marker != -1:\n",
        "            start_of_answer_idx = start_of_answer_marker + len(answer_prefix)\n",
        "            predicted_answer = generated_text_full[start_of_answer_idx:].strip()\n",
        "            if predicted_answer.endswith(eval_tokenizer.eos_token):\n",
        "                predicted_answer = predicted_answer[:-len(eval_tokenizer.eos_token)].strip()\n",
        "        else:\n",
        "            if generated_text_full.startswith(prompt):\n",
        "                 predicted_answer = generated_text_full[len(prompt):].strip()\n",
        "                 if predicted_answer.endswith(eval_tokenizer.eos_token):\n",
        "                    predicted_answer = predicted_answer[:-len(eval_tokenizer.eos_token)].strip()\n",
        "            else:\n",
        "                predicted_answer = eval_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "                if predicted_answer.startswith(question):\n",
        "                    predicted_answer = predicted_answer[len(question):].strip().lstrip(':').strip()\n",
        "\n",
        "        predictions.append(predicted_answer if predicted_answer else \" \")\n",
        "        references.append(reference_answer if reference_answer else \" \")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Exception: '{question}'. ERROR: {e}\")\n",
        "        predictions.append(\" \")\n",
        "        references.append(reference_answer if reference_answer else \" \")\n",
        "\n",
        "\n",
        "if predictions and references:\n",
        "    print(\"\\nCalculating ROUGE scores...\")\n",
        "    rouge_results = rouge_metric.compute(predictions=predictions, references=references)\n",
        "    print(\"\\nROUGE reusults:\")\n",
        "    for key, value in rouge_results.items():\n",
        "        print(f\"{key}: {value*100:.2f}\")\n",
        "else:\n",
        "    print(\"There is no prediction or reference to calculate ROUGE.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAg4V7xfr6a0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title 9. Gradio interface (Generative QA use Sentence Transformers)\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model_st_name = 'all-mpnet-base-v2'\n",
        "print(f\"Downloading  Sentence Transformer: {model_st_name}...\")\n",
        "try:\n",
        "    st_model = SentenceTransformer(model_st_name)\n",
        "    print(\" Sentence Transformer download completed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Exception: {e}. .\")\n",
        "    st_model = None\n",
        "\n",
        "\n",
        "if 'processed_data_generative' not in globals() or not processed_data_generative:\n",
        "    print(\"Warning: processed_data_generative null.\")\n",
        "    knowledge_base_df_gen = pd.DataFrame([\n",
        "        {\"id\": \"syn_kb_0\", \"question\": \"What are the symptoms of flu?\", \"answer\": \"Flu is a common respiratory illness caused by influenza viruses.\"},\n",
        "        {\"id\": \"syn_kb_1\", \"question\": \"How can one treat a common cold?\", \"answer\": \"For a common cold, it's advisable to get plenty of rest, drink fluids, and use over-the-counter medications for symptoms.\"},\n",
        "        {\"id\": \"syn_kb_2\", \"question\": \"Tell me about MRI scans.\", \"answer\": \"MRI, or Magnetic Resonance Imaging, is a medical imaging technique used to form pictures of the anatomy and physiological processes of the body.\"}\n",
        "    ])\n",
        "else:\n",
        "    knowledge_base_df_gen = pd.DataFrame(processed_data_generative)\n",
        "\n",
        "corpus_questions = []\n",
        "corpus_answers = []\n",
        "corpus_embeddings = None\n",
        "\n",
        "if st_model and not knowledge_base_df_gen.empty:\n",
        "    corpus_questions = knowledge_base_df_gen['question'].astype(str).tolist()\n",
        "    corpus_answers = knowledge_base_df_gen['answer'].astype(str).tolist()\n",
        "    if corpus_questions:\n",
        "        print(f\"Calculating embeddings for {len(corpus_questions)} questions in corpus...\")\n",
        "        try:\n",
        "            if torch.cuda.is_available():\n",
        "                st_model.to(device)\n",
        "                print(f\"Sentence Transformer model moved to {device}.\")\n",
        "\n",
        "            corpus_embeddings = st_model.encode(corpus_questions, convert_to_tensor=True, show_progress_bar=True)\n",
        "            print(\"Done! Embeddings for corpus.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Exception calculate embeddings cho corpus: {e}\")\n",
        "            corpus_embeddings = None # Đặt là None nếu có lỗi\n",
        "    else:\n",
        "        print(\"No questions in corpus for calculate embeddings.\")\n",
        "else:\n",
        "    if not st_model:\n",
        "        print(\"Can not calculate embeddings cause Sentence Transformer has not download yet.\")\n",
        "    if knowledge_base_df_gen.empty:\n",
        "        print(\"Knowledge base null, Fail embeddings.\")\n",
        "\n",
        "if 'trainer_gen' not in globals() or not hasattr(trainer_gen, 'model'):\n",
        "    print(\"CẢNH BÁO: trainer_gen or trainer_gen.model not found..\")\n",
        "\n",
        "    try:\n",
        "        output_model_dir_gen = \"./fine_tuned_medquad_gpt2_generative_final\"\n",
        "        gen_qa_model = GPT2LMHeadModel.from_pretrained(output_model_dir_gen)\n",
        "        gen_qa_tokenizer = GPT2TokenizerFast.from_pretrained(output_model_dir_gen)\n",
        "        gen_qa_model.to(device)\n",
        "        print(f\"Redownload model {output_model_dir_gen}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Exception model generative QA: {e}.\")\n",
        "        gen_qa_model = None\n",
        "        gen_qa_tokenizer = None\n",
        "else:\n",
        "    gen_qa_model = trainer_gen.model\n",
        "    gen_qa_tokenizer = tokenizer\n",
        "\n",
        "if gen_qa_model:\n",
        "    gen_qa_model.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def answer_question_gradio_generative(user_question):\n",
        "    print(f\"\\nReceived question from Gradio (Generative): {user_question}\")\n",
        "    if not user_question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "\n",
        "    # Step 1: Find a similar Q&A in the \"database\" using Sentence Transformers\n",
        "    similar_q, similar_a, similarity_score = find_most_similar_qa_st(user_question)\n",
        "    similarity_info = \"\"\n",
        "    if similar_q:\n",
        "        similarity_info = (\n",
        "            f\"\\n\\n--- Reference Information from Data (Semantic Similarity: {similarity_score:.2f}) ---\\n\"\n",
        "            f\"Most similar question:\\n{similar_q}\\n\"\n",
        "            f\"Corresponding answer:\\n{similar_a}\\n\"\n",
        "            f\"-------------------------------------------------------------------\"\n",
        "        )\n",
        "        print(f\"Found similar Q&A (semantically) with similarity score {similarity_score:.2f}\")\n",
        "\n",
        "    # Step 2: Generate an answer using the generative QA model\n",
        "    if not gen_qa_model or not gen_qa_tokenizer:\n",
        "        return \"Error: Generative QA model is not loaded.\"\n",
        "\n",
        "    prompt = f\"Question: {user_question} Answer:\"\n",
        "    inputs = gen_qa_tokenizer(prompt, return_tensors=\"pt\").to(device if gen_qa_model.device.type == 'cuda' else 'cpu')  # Ensure input is on the same device as the model\n",
        "\n",
        "    generated_answer_text = \"Unable to generate an answer.\"\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            outputs = gen_qa_model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_length=200,  # Slightly increased to allow longer responses\n",
        "                num_beams=1,\n",
        "                no_repeat_ngram_size=2,  # Helps avoid phrase repetition\n",
        "                early_stopping=True,\n",
        "                eos_token_id=gen_qa_tokenizer.eos_token_id,\n",
        "                pad_token_id=gen_qa_tokenizer.pad_token_id,\n",
        "                temperature=0.3,  # Controls randomness\n",
        "                do_sample=True,  # MUST HAVE\n",
        "                # top_k=50  # Recommended to add top_k or top_p to prevent strange token generation\n",
        "                top_p=0.95\n",
        "            )\n",
        "\n",
        "        generated_text_full = gen_qa_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "        answer_prefix = \"Answer:\"\n",
        "        # Locate the last occurrence of \"Answer:\" to ensure proper extraction if prompt contains \"Answer:\"\n",
        "        start_of_answer_marker = generated_text_full.rfind(answer_prefix)\n",
        "\n",
        "        if start_of_answer_marker != -1:\n",
        "            start_of_answer_idx = start_of_answer_marker + len(answer_prefix)\n",
        "            generated_answer_text = generated_text_full[start_of_answer_idx:].strip()\n",
        "            # Remove eos_token if present at the end\n",
        "            if generated_answer_text.endswith(gen_qa_tokenizer.eos_token):\n",
        "                generated_answer_text = generated_answer_text[:-len(gen_qa_tokenizer.eos_token)].strip()\n",
        "        else:  # Fallback if \"Answer:\" is not found\n",
        "            if generated_text_full.startswith(prompt):  # If output starts with prompt\n",
        "                generated_answer_text = generated_text_full[len(prompt):].strip()\n",
        "                if generated_answer_text.endswith(gen_qa_tokenizer.eos_token):\n",
        "                    generated_answer_text = generated_answer_text[:-len(gen_qa_tokenizer.eos_token)].strip()\n",
        "            else:  # Otherwise, extract the full output excluding special tokens\n",
        "                generated_answer_text = gen_qa_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "                # Sometimes the prompt might still be present, remove it if user_question is included\n",
        "                if generated_answer_text.startswith(user_question):\n",
        "                    generated_answer_text = generated_answer_text[len(user_question):].strip().lstrip(':').strip()\n",
        "\n",
        "        print(f\"Generated answer: {generated_answer_text}\")\n",
        "\n",
        "        final_response = f\"Medical Chatbot (GPT-2 Generative) Answer:\\n{generated_answer_text}\"\n",
        "        final_response += similarity_info\n",
        "\n",
        "        return final_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating answer: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        error_response = f\"An error occurred while generating the answer. {e}\"\n",
        "        error_response += similarity_info\n",
        "        return error_response\n",
        "\n",
        "# Create Gradio Interface\n",
        "iface_gen = gr.Interface(\n",
        "    fn=answer_question_gradio_generative,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Enter your medical question here... Example: 'What are common treatments for high blood pressure?'\"),\n",
        "    outputs=gr.Text(label=\"Answer and Reference Information\"),\n",
        "    title=\"Medical Q&A Chatbot (GPT-2 Generative + Semantic Search)\",\n",
        "    description=\"Enter a medical question. The chatbot will generate an answer and display the most semantically similar Q&A pair from the learned dataset.\",\n",
        "    examples=[\n",
        "        [\"What are the symptoms of flu?\"],\n",
        "        [\"How to treat a common cold?\"],\n",
        "        [\"What is an MRI?\"],\n",
        "        [\"Tell me about diabetes type 2.\"],\n",
        "        [\"What are common treatments for high blood pressure?\"]\n",
        "    ],\n",
        "    allow_flagging='never' # Disable flagging if not needed\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nĐang khởi chạy giao diện Gradio cho Generative QA (có tìm kiếm ngữ nghĩa)...\")\n",
        "iface_gen.launch(debug=True, share=True) # Chạy với debug=True nếu cần xem log chi tiết\n",
        "# iface_gen.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}